DDL-Net-Sign-Language-Recognition
Indian Sign Language (ISL) is a visual form of communication used by the deaf
community in India. Recognizing ISL characters is an important step towards
developing technology. With the advancement of computer vision and machine
learning, researchers have been exploring various techniques for recognizing ISL
characters to build systems that can accurately interpret and
transcribe sign language gestures into text. This study presents an in-depth exploration
of a deep learning-based architecture named DDLNet, derived from the
Densenet121 model, tailored for the extraction of both low-level and high-level
features, as well as the classification of sign language characters. To facilitate
this research, a novel dataset known as the National Institute of Technology
Jalandhar-Indian Sign Language Characters (NITJ-ISLC) is introduced, encompassing
approximately 10,800 images distributed across 36 classes, spanning from
0 to 9 and A to Z. These images feature a uniform, plain background for consistency.
For comparative analysis, deep learning models is employed,
including VGG16, VGG19, ResNet50, Inception V3, Xception, DenseNet, and
MobileNet, training them for recognition using the NITJ-ISLC dataset. The
visual results are comprehensively evaluated, revealing that the DDLNet model
excels, reaching an outperformed and remarkable performance on the NITJISLC
dataset. Furthermore, the DDLNet model is tested on various benchmark
datasets, consistently yielding superior performance.

Currently, we have provided partial codes. However, the complete code will be provided after the acceptance notification of the paper.

Some of the test dataset is uploaded and the full dataset generated during or analyzed during the current study is available from the corresponding author on
reasonable request.
